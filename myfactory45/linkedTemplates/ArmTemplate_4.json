{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory name",
			"defaultValue": "myfactory45"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/storagetrigger')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Copy data1",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "storagevent",
								"type": "DatasetReference",
								"parameters": {
									"fname": {
										"value": "@pipeline().parameters.fname",
										"type": "Expression"
									}
								}
							}
						],
						"outputs": [
							{
								"referenceName": "storageeventsink",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"fname": {
						"type": "string"
					}
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/csv2tbl_cdc')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "incrementalcsv",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "fromcdccsv",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          eid as string,",
						"          doj as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     enableCdc: true,",
						"     mode: 'read',",
						"     skipInitialLoad: false,",
						"     rowUrlColumn: 'filepath') ~> source1",
						"source1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow2')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "dataflowdept",
								"type": "DatasetReference"
							},
							"name": "dept"
						},
						{
							"dataset": {
								"referenceName": "dataflowemp",
								"type": "DatasetReference"
							},
							"name": "emp"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSqlTable1",
								"type": "DatasetReference"
							},
							"name": "sink1"
						},
						{
							"dataset": {
								"referenceName": "AzureSqlTable1",
								"type": "DatasetReference"
							},
							"name": "sink2"
						}
					],
					"transformations": [
						{
							"name": "Renaming"
						},
						{
							"name": "AddingTargetrevenue"
						},
						{
							"name": "join1"
						},
						{
							"name": "select1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          dept_id as integer,",
						"          dname as string,",
						"          totalrevenue as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> dept",
						"source(output(",
						"          empid as integer,",
						"          ename as string,",
						"          salary as integer,",
						"          did as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> emp",
						"emp select(mapColumn(",
						"          eid = empid,",
						"          ename,",
						"          salary,",
						"          did",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> Renaming",
						"dept derive(TargetRevenue = totalrevenue+(totalrevenue*20/100),",
						"          dname = initCap(dname)) ~> AddingTargetrevenue",
						"Renaming, AddingTargetrevenue join(did == dept_id,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> join1",
						"join1 select(mapColumn(",
						"          eid,",
						"          ename,",
						"          salary,",
						"          dname",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          eid,",
						"          ename,",
						"          salary,",
						"          dname",
						"     )) ~> sink1",
						"AddingTargetrevenue sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          dept_id,",
						"          dname,",
						"          Totalrevenue = totalrevenue,",
						"          tragetrevenue = TargetRevenue",
						"     )) ~> sink2"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow3')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "batch2dept",
								"type": "DatasetReference"
							},
							"name": "dept"
						},
						{
							"dataset": {
								"referenceName": "batch2emp",
								"type": "DatasetReference"
							},
							"name": "emp"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSqlTable1",
								"type": "DatasetReference"
							},
							"name": "sink1"
						},
						{
							"dataset": {
								"referenceName": "high",
								"type": "DatasetReference"
							},
							"name": "sink2"
						},
						{
							"dataset": {
								"referenceName": "low",
								"type": "DatasetReference"
							},
							"name": "sink3"
						}
					],
					"transformations": [
						{
							"name": "chnagethedept"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "select1"
						},
						{
							"name": "join1"
						},
						{
							"name": "select2"
						},
						{
							"name": "split1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          dept_id as integer,",
						"          dname as string,",
						"          totalrevenue as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> dept",
						"source(output(",
						"          empid as integer,",
						"          ename as string,",
						"          salary as integer,",
						"          did as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> emp",
						"dept derive(dname = initCap(dname),",
						"          targetrevenue = totalrevenue+(totalrevenue*20/100)) ~> chnagethedept",
						"emp derive(ename = initCap(ename)) ~> derivedColumn1",
						"derivedColumn1 select(mapColumn(",
						"          eid = empid,",
						"          ename,",
						"          salary,",
						"          did",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1, dept join(did == dept_id,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> join1",
						"join1 select(mapColumn(",
						"          eid,",
						"          ename,",
						"          salary,",
						"          dname",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select2",
						"chnagethedept split(totalrevenue>=40000,",
						"     disjoint: false) ~> split1@(high, low)",
						"select2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sink1",
						"split1@high sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          did as integer,",
						"          dname as string,",
						"          revnue as integer,",
						"          targetrev as integer",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          did = dept_id,",
						"          dname,",
						"          revnue = totalrevenue,",
						"          targetrev = targetrevenue",
						"     )) ~> sink2",
						"split1@low sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          did as integer,",
						"          dname as string,",
						"          revnue as integer,",
						"          targetrev as integer",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          did = dept_id,",
						"          dname,",
						"          revnue = totalrevenue,",
						"          targetrev = targetrevenue",
						"     )) ~> sink3"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow4')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "input_folder",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "output_folder",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [],
					"scriptLines": [
						"parameters{",
						"     dfname as string",
						"}",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     wildcardPaths:[(concat('data/input/', $dfname))]) ~> source1",
						"source1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:[($dfname)],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow5')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "tablewithcondition",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSqlTable1",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "filter1"
						}
					],
					"scriptLines": [
						"parameters{",
						"     price as integer",
						"}",
						"source(output(",
						"          bookid as integer,",
						"          bookname as string,",
						"          bookprice as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> source1",
						"source1 filter(bookprice>=$price) ~> filter1",
						"filter1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow6')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "AzureSqlTable5",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSqlTable6",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          id as integer,",
						"          name1 as string,",
						"          date1 as timestamp",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     enableCdc: true,",
						"     mode: 'read',",
						"     skipInitialLoad: false,",
						"     waterMarkColumn: 'date1',",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> source1",
						"source1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          id as integer,",
						"          name1 as string,",
						"          date1 as timestamp",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow7')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "batch2csvcdcsource",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSqlTable8",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          eid as integer,",
						"          doj as date",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     enableCdc: true,",
						"     mode: 'read',",
						"     skipInitialLoad: false,",
						"     rowUrlColumn: 'file1') ~> source1",
						"source1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow8')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "batch2csvcdcsource",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DelimitedText2",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          eid as string,",
						"          doj as date",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          cid as string,",
						"          cname as string",
						"     ),",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/incrementaldataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "source_cust",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "target_cust",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "derivedColumn1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          cudtid as integer,",
						"          custname as string,",
						"          doi as timestamp",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     enableCdc: true,",
						"     mode: 'read',",
						"     skipInitialLoad: false,",
						"     waterMarkColumn: 'doi',",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> source1",
						"source1 derive(custname = upper(custname)) ~> derivedColumn1",
						"derivedColumn1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          cudtid as integer,",
						"          custname as string,",
						"          doi as timestamp",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/scd_dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "emp_new",
								"type": "DatasetReference"
							},
							"name": "empnewcsv"
						},
						{
							"dataset": {
								"referenceName": "AzureSqlTablescd",
								"type": "DatasetReference"
							},
							"name": "dimempscd"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSqlTablescd",
								"type": "DatasetReference"
							},
							"name": "sink1"
						},
						{
							"dataset": {
								"referenceName": "AzureSqlTablescd",
								"type": "DatasetReference"
							},
							"name": "sink2"
						}
					],
					"transformations": [
						{
							"name": "hashempcsv"
						},
						{
							"name": "hashdimemp"
						},
						{
							"name": "exists1"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "exists2"
						},
						{
							"name": "derivedColumn2"
						},
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as integer,",
						"          ename as string,",
						"          salary as integer,",
						"          dname as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> empnewcsv",
						"source(output(",
						"          eid as integer,",
						"          ename as string,",
						"          salary as integer,",
						"          dname as string,",
						"          isactive as integer,",
						"          startdate as timestamp,",
						"          enddate as timestamp",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> dimempscd",
						"empnewcsv derive(hashempcsv1 = md5(ename,salary,dname)) ~> hashempcsv",
						"dimempscd derive(hash1 = md5(ename,salary,dname)) ~> hashdimemp",
						"hashempcsv, hashdimemp exists(hashempcsv1 == hash1,",
						"     negate:true,",
						"     broadcast: 'auto')~> exists1",
						"exists1 derive(isactive = 1,",
						"          startdate = currentDate(),",
						"          enddate = toDate('9999-1-1')) ~> derivedColumn1",
						"hashdimemp, derivedColumn1 exists(eid == empid",
						"     && dimempscd@ename == empnewcsv@ename,",
						"     negate:false,",
						"     broadcast: 'auto')~> exists2",
						"exists2 derive(isactive = 0,",
						"          enddate = currentDate()) ~> derivedColumn2",
						"derivedColumn2 alterRow(updateIf(true())) ~> alterRow1",
						"derivedColumn1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          eid as integer,",
						"          ename as string,",
						"          salary as integer,",
						"          dname as string,",
						"          isactive as integer,",
						"          startdate as timestamp,",
						"          enddate as timestamp",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          eid = empid,",
						"          ename,",
						"          salary,",
						"          dname,",
						"          isactive,",
						"          startdate,",
						"          enddate",
						"     )) ~> sink1",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          eid as integer,",
						"          ename as string,",
						"          salary as integer,",
						"          dname as string,",
						"          isactive as integer,",
						"          startdate as timestamp,",
						"          enddate as timestamp",
						"     ),",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:true,",
						"     keys:['eid','ename'],",
						"     skipKeyWrites:true,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          eid,",
						"          ename,",
						"          salary,",
						"          dname,",
						"          isactive,",
						"          startdate,",
						"          enddate",
						"     )) ~> sink2"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Dataflow_pipe')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "dataflow2",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"dept": {},
									"emp": {},
									"sink1": {
										"sourcename": "gdg"
									},
									"sink2": {
										"sourcename": "tr"
									}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "till Day 9"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/dataflow2')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/MetadataDrivenCopyTask_a3e_MiddleLevel')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "This pipeline will copy one batch of objects. The objects belonging to this batch will be copied parallelly.",
				"activities": [
					{
						"name": "DivideOneBatchIntoMultipleGroups",
						"description": "Divide objects from single batch into multiple sub parallel groups to avoid reaching the output limit of lookup activity.",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@range(0, add(div(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity),\n                    if(equals(mod(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity), 0), 0, 1)))",
								"type": "Expression"
							},
							"isSequential": false,
							"batchCount": 50,
							"activities": [
								{
									"name": "GetObjectsPerGroupToCopy",
									"description": "Get objects (tables etc.) from control table required to be copied in this group. The order of objects to be copied following the TaskId in control table (ORDER BY [TaskId] DESC).",
									"type": "Lookup",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "AzureSqlSource",
											"sqlReaderQuery": {
												"value": "WITH OrderedControlTable AS (\n                             SELECT *, ROW_NUMBER() OVER (ORDER BY [TaskId], [Id] DESC) AS RowNumber\n                             FROM @{pipeline().parameters.MainControlTableName}\n                             where TopLevelPipelineName = '@{pipeline().parameters.TopLevelPipelineName}'\n                             and TriggerName like '%@{pipeline().parameters.TriggerName}%' and CopyEnabled = 1)\n                             SELECT * FROM OrderedControlTable WHERE RowNumber BETWEEN @{add(mul(int(item()),pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity),\n                             add(mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.CurrentSequentialNumberOfBatch), 1))}\n                             AND @{min(add(mul(int(item()), pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity), add(mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.CurrentSequentialNumberOfBatch),\n                             pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity)),\n                            mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, add(pipeline().parameters.CurrentSequentialNumberOfBatch,1)), pipeline().parameters.SumOfObjectsToCopy)}",
												"type": "Expression"
											},
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "MetadataDrivenCopyTask_a3e_ControlDS",
											"type": "DatasetReference",
											"parameters": {}
										},
										"firstRowOnly": false
									}
								},
								{
									"name": "CopyObjectsInOneGroup",
									"description": "Execute another pipeline to copy objects from one group. The objects belonging to this group will be copied parallelly.",
									"type": "ExecutePipeline",
									"dependsOn": [
										{
											"activity": "GetObjectsPerGroupToCopy",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "MetadataDrivenCopyTask_a3e_BottomLevel",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"ObjectsPerGroupToCopy": {
												"value": "@activity('GetObjectsPerGroupToCopy').output.value",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"MaxNumberOfObjectsReturnedFromLookupActivity": {
						"type": "Int"
					},
					"TopLevelPipelineName": {
						"type": "String"
					},
					"TriggerName": {
						"type": "String"
					},
					"CurrentSequentialNumberOfBatch": {
						"type": "Int"
					},
					"SumOfObjectsToCopy": {
						"type": "Int"
					},
					"SumOfObjectsToCopyForCurrentBatch": {
						"type": "Int"
					},
					"MainControlTableName": {
						"type": "String"
					}
				},
				"folder": {
					"name": "MetadataDrivenCopyTask_a3e_20221031"
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/MetadataDrivenCopyTask_dl2dl_l3i_MiddleLevel')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "This pipeline will copy one batch of objects. The objects belonging to this batch will be copied parallelly.",
				"activities": [
					{
						"name": "DivideOneBatchIntoMultipleGroups",
						"description": "Divide objects from single batch into multiple sub parallel groups to avoid reaching the output limit of lookup activity.",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@range(0, add(div(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity),\n                    if(equals(mod(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity), 0), 0, 1)))",
								"type": "Expression"
							},
							"isSequential": false,
							"batchCount": 50,
							"activities": [
								{
									"name": "GetObjectsPerGroupToCopy",
									"description": "Get objects (tables etc.) from control table required to be copied in this group. The order of objects to be copied following the TaskId in control table (ORDER BY [TaskId] DESC).",
									"type": "Lookup",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "AzureSqlSource",
											"sqlReaderQuery": {
												"value": "WITH OrderedControlTable AS (\n                             SELECT *, ROW_NUMBER() OVER (ORDER BY [TaskId], [Id] DESC) AS RowNumber\n                             FROM @{pipeline().parameters.MainControlTableName}\n                             where TopLevelPipelineName = '@{pipeline().parameters.TopLevelPipelineName}'\n                             and TriggerName like '%@{pipeline().parameters.TriggerName}%' and CopyEnabled = 1)\n                             SELECT * FROM OrderedControlTable WHERE RowNumber BETWEEN @{add(mul(int(item()),pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity),\n                             add(mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.CurrentSequentialNumberOfBatch), 1))}\n                             AND @{min(add(mul(int(item()), pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity), add(mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.CurrentSequentialNumberOfBatch),\n                             pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity)),\n                            mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, add(pipeline().parameters.CurrentSequentialNumberOfBatch,1)), pipeline().parameters.SumOfObjectsToCopy)}",
												"type": "Expression"
											},
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "MetadataDrivenCopyTask_dl2dl_l3i_ControlDS",
											"type": "DatasetReference",
											"parameters": {}
										},
										"firstRowOnly": false
									}
								},
								{
									"name": "CopyObjectsInOneGroup",
									"description": "Execute another pipeline to copy objects from one group. The objects belonging to this group will be copied parallelly.",
									"type": "ExecutePipeline",
									"dependsOn": [
										{
											"activity": "GetObjectsPerGroupToCopy",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "MetadataDrivenCopyTask_dl2dl_l3i_BottomLevel",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"ObjectsPerGroupToCopy": {
												"value": "@activity('GetObjectsPerGroupToCopy').output.value",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"MaxNumberOfObjectsReturnedFromLookupActivity": {
						"type": "Int"
					},
					"TopLevelPipelineName": {
						"type": "String"
					},
					"TriggerName": {
						"type": "String"
					},
					"CurrentSequentialNumberOfBatch": {
						"type": "Int"
					},
					"SumOfObjectsToCopy": {
						"type": "Int"
					},
					"SumOfObjectsToCopyForCurrentBatch": {
						"type": "Int"
					},
					"MainControlTableName": {
						"type": "String"
					}
				},
				"folder": {
					"name": "MetadataDrivenCopyTask_dl2dl_l3i_20221009"
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/MetadataDrivenCopyTask_t8w_MiddleLevel')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "This pipeline will copy one batch of objects. The objects belonging to this batch will be copied parallelly.",
				"activities": [
					{
						"name": "DivideOneBatchIntoMultipleGroups",
						"description": "Divide objects from single batch into multiple sub parallel groups to avoid reaching the output limit of lookup activity.",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@range(0, add(div(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity),\n                    if(equals(mod(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity), 0), 0, 1)))",
								"type": "Expression"
							},
							"isSequential": false,
							"batchCount": 50,
							"activities": [
								{
									"name": "GetObjectsPerGroupToCopy",
									"description": "Get objects (tables etc.) from control table required to be copied in this group. The order of objects to be copied following the TaskId in control table (ORDER BY [TaskId] DESC).",
									"type": "Lookup",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "AzureSqlSource",
											"sqlReaderQuery": {
												"value": "WITH OrderedControlTable AS (\n                             SELECT *, ROW_NUMBER() OVER (ORDER BY [TaskId], [Id] DESC) AS RowNumber\n                             FROM @{pipeline().parameters.MainControlTableName}\n                             where TopLevelPipelineName = '@{pipeline().parameters.TopLevelPipelineName}'\n                             and TriggerName like '%@{pipeline().parameters.TriggerName}%' and CopyEnabled = 1)\n                             SELECT * FROM OrderedControlTable WHERE RowNumber BETWEEN @{add(mul(int(item()),pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity),\n                             add(mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.CurrentSequentialNumberOfBatch), 1))}\n                             AND @{min(add(mul(int(item()), pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity), add(mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.CurrentSequentialNumberOfBatch),\n                             pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity)),\n                            mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, add(pipeline().parameters.CurrentSequentialNumberOfBatch,1)), pipeline().parameters.SumOfObjectsToCopy)}",
												"type": "Expression"
											},
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "MetadataDrivenCopyTask_t8w_ControlDS",
											"type": "DatasetReference",
											"parameters": {}
										},
										"firstRowOnly": false
									}
								},
								{
									"name": "CopyObjectsInOneGroup",
									"description": "Execute another pipeline to copy objects from one group. The objects belonging to this group will be copied parallelly.",
									"type": "ExecutePipeline",
									"dependsOn": [
										{
											"activity": "GetObjectsPerGroupToCopy",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "MetadataDrivenCopyTask_t8w_BottomLevel",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"ObjectsPerGroupToCopy": {
												"value": "@activity('GetObjectsPerGroupToCopy').output.value",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"MaxNumberOfObjectsReturnedFromLookupActivity": {
						"type": "Int"
					},
					"TopLevelPipelineName": {
						"type": "String"
					},
					"TriggerName": {
						"type": "String"
					},
					"CurrentSequentialNumberOfBatch": {
						"type": "Int"
					},
					"SumOfObjectsToCopy": {
						"type": "Int"
					},
					"SumOfObjectsToCopyForCurrentBatch": {
						"type": "Int"
					},
					"MainControlTableName": {
						"type": "String"
					}
				},
				"folder": {
					"name": "MetadataDrivenCopyTask_t8w_20221010"
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/batch2df')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "dataflow3",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"dept": {},
									"emp": {},
									"sink1": {
										"sourcename": "ytrs"
									},
									"sink2": {},
									"sink3": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "till Day 9"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/dataflow3')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/calldfwithparameter')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Get Metadata1",
						"type": "GetMetadata",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "input_folder",
								"type": "DatasetReference",
								"parameters": {}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "DelimitedTextReadSettings"
							}
						}
					},
					{
						"name": "ForEach1",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get Metadata1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get Metadata1').output.childItems",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Data flow1",
									"type": "ExecuteDataFlow",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"dataflow": {
											"referenceName": "dataflow4",
											"type": "DataFlowReference",
											"parameters": {
												"dfname": {
													"value": "'@{item().name}'",
													"type": "Expression"
												}
											},
											"datasetParameters": {
												"source1": {},
												"sink1": {}
											}
										},
										"staging": {},
										"compute": {
											"coreCount": 8,
											"computeType": "General"
										},
										"traceLevel": "Fine"
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "till Day 9"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/dataflow4')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/csv2tablcdc')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "incrementaldataflow",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine",
							"continuationSettings": {
								"customizedCheckpointKey": "60f2fc30-e462-420a-b711-302d3d0e8e4d"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/incrementaldataflow')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/incrementalwithtran')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "incrementaldataflow",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine",
							"continuationSettings": {
								"customizedCheckpointKey": "74b50591-3c92-43b3-9970-17a28b3edb96"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/incrementaldataflow')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/pipeline1')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "dataflow5",
								"type": "DataFlowReference",
								"parameters": {
									"price": {
										"value": "@pipeline().parameters.price",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"source1": {},
									"sink1": {
										"sourcename": "fdgdfd"
									}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"price": {
						"type": "int"
					}
				},
				"folder": {
					"name": "till Day 9"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/dataflow5')]"
			]
		}
	]
}